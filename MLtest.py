# --- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 0: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ---
# pip install pandas scikit-learn matplotlib seaborn wordcloud nltk tensorflow
 
# ----------------------------------------
# --- import library ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ---
import pandas as pd
import re
import sys
import warnings
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import numpy as np
 
# --- Import NLTK (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©) ---
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
 
# --- Import libraries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning (TensorFlow/Keras) ---
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense, Embedding, LSTM, Bidirectional,
    GlobalMaxPooling1D, Conv1D, Dropout, SpatialDropout1D, GRU
)
from tensorflow.keras.optimizers import Adam
 
# --- Import ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ---
from sklearn.metrics import classification_report, confusion_matrix
 
# --- ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á NLTK (‡∏ó‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î 'punkt' (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö word_tokenize) ---")
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏∏‡∏Å‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏à‡∏∞‡∏°‡∏µ 'punkt_tab' ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
    try:
        print("--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î 'punkt_tab' ---")
        nltk.download('punkt_tab')
    except Exception:
        pass
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("--- ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î 'stopwords' (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥) ---")
    nltk.download('stopwords')
 
# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
warnings.filterwarnings('ignore')
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (DATA PREPARATION)
# ==============================================================================
 
# --- 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡πâ‡∏ß‡∏¢ pandas ---
print("--- 1. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV ---")
FPath = "D:\‡∏á‡∏≤‡∏ô\MLPro/Amazon_Unlocked_Mobile.csv"  # <--- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Path ‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
 
try:
    df_raw = pd.read_csv(FPath)
    df = df_raw[['Reviews', 'Rating']].copy()
    df = df.dropna(subset=['Reviews'])
    df = df.reset_index(drop=True)
    print("--- 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ---")
except FileNotFoundError:
    print(f"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Dataset ‡∏ó‡∏µ‡πà Path: {FPath}")
    sys.exit()
except Exception as e:
    print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
    sys.exit()
 
# --- 6. ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å/‡∏™‡∏£‡πâ‡∏≤‡∏á Label ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ---
def map_sentiment(rating):
    if rating >= 4:
        return 'positive'
    elif rating <= 2:
        return 'negative'
    return None
 
df['sentiment'] = df['Rating'].apply(map_sentiment)
df = df.dropna(subset=['sentiment'])
df = df.reset_index(drop=True)
 
# --- 2, 3, 4, 5. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Pipeline) [‡∏â‡∏ö‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©] ---
print("\n--- 2,3,4,5. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)... ---")
stop_words_eng = set(stopwords.words('english'))
 
def clean_text_pipeline_eng(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    cleaned_tokens = [
        word for word in tokens
        if word.isalpha() and len(word) > 1 and word not in stop_words_eng
    ]
    return " ".join(cleaned_tokens)
 
df['text_clean'] = df['Reviews'].apply(clean_text_pipeline_eng)
 
print("--- 2,3,4,5. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ---")
print(df[['text_clean', 'sentiment']].head())
 
# --- 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏• (Imbalance) ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
print("\n--- 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---")
print(df['sentiment'].value_counts())
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (EDA)
# ==============================================================================
 
print("\n\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Exploratory Data Analysis (EDA) ---")
sns.set(style="whitegrid")
print("--- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Font (Default) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ---")
 
 
# --- EDA 2.1: ‡∏Å‡∏£‡∏≤‡∏ü‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ (Class Distribution) ---
print("\n--- EDA 2.1: ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ ---")
plt.figure(figsize=(8, 5))
sns.countplot(x='sentiment', data=df, palette=['#34A853', '#EA4335'])
plt.title('Distribution of Sentiments (Positive vs Negative)')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()
 
# --- EDA 2.2: ‡∏Å‡∏£‡∏≤‡∏ü‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ---
print("\n--- EDA 2.2: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ---")
df['word_count'] = df['text_clean'].apply(lambda x: len(x.split()))
print("Word Count Statistics:")
print(df['word_count'].describe())
 
plt.figure(figsize=(12, 6))
sns.histplot(df['word_count'], bins=50, kde=True)
plt.title('Distribution of Word Count in Reviews (Cleaned)')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.xlim(0, 150)
plt.show()
 
# -------------------------------------------------------------------------------
# --- EDA 3.1: Word Clouds (‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢) [‡∏â‡∏ö‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© V2] ---
# -------------------------------------------------------------------------------
print("\n--- EDA 3.1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Word Clouds (‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á)... ---")
text_positive = " ".join(review for review in df[df['sentiment'] == 'positive']['text_clean'])
text_negative = " ".join(review for review in df[df['sentiment'] == 'negative']['text_clean'])
 
try:
    wordcloud_pos = WordCloud(
        width=1200, height=600,
        background_color='white',
        colormap='viridis',
        prefer_horizontal=0.6,
        min_font_size=10,
        max_words=200,
        random_state=42
    ).generate(text_positive)
 
    wordcloud_neg = WordCloud(
        width=1200, height=600,
        background_color='white',
        colormap='plasma',
        prefer_horizontal=0.6,
        min_font_size=10,
        max_words=200,
        random_state=42
    ).generate(text_negative)
 
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 12))
    ax1.imshow(wordcloud_pos, interpolation='bilinear')
    ax1.set_title('Word Cloud - Positive Reviews', fontsize=24)
    ax1.axis('off')
    ax2.imshow(wordcloud_neg, interpolation='bilinear')
    ax2.set_title('Word Cloud - Negative Reviews', fontsize=24)
    ax2.axis('off')
    plt.show()
    print("--- ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Clouds ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ---")
except Exception as e:
    print(f"--- ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud: {e} ---")
 
 
# -------------------------------------------------------------------------------
# --- EDA 3.2: ‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ (Word Frequency Graph) ---
# -------------------------------------------------------------------------------
print("\n--- EDA 3.2: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ Top 20 ---")
 
def get_top_n_words(corpus, n=None):
    all_words = " ".join(corpus).split()
    word_counts = Counter(all_words)
    top_n_words = word_counts.most_common(n)
    return top_n_words
 
top_words_positive = get_top_n_words(df[df['sentiment'] == 'positive']['text_clean'], n=20)
top_words_negative = get_top_n_words(df[df['sentiment'] == 'negative']['text_clean'], n=20)
 
df_top_pos = pd.DataFrame(top_words_positive, columns=['word', 'count'])
df_top_neg = pd.DataFrame(top_words_negative, columns=['word', 'count'])
 
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))
sns.barplot(x='count', y='word', data=df_top_pos, ax=ax1, palette='Greens_r')
ax1.set_title('Top 20 Words in Positive Reviews')
ax1.set_xlabel('Count')
ax1.set_ylabel('Word')
sns.barplot(x='count', y='word', data=df_top_neg, ax=ax2, palette='Reds_r')
ax2.set_title('Top 20 Words in Negative Reviews')
ax2.set_xlabel('Count')
ax2.set_ylabel('Word')
plt.tight_layout()
plt.show()
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (DATA SPLITTING)
# ==============================================================================
 
print("\n\n--- 3. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test ---")
X = df['text_clean']
y = df['sentiment']
 
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
 
print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train (X_train): {len(X_train)} ‡πÅ‡∏ñ‡∏ß")
print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Test (X_test): {len(X_test)} ‡πÅ‡∏ñ‡∏ß")
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DEEP LEARNING
# ==============================================================================
 
# --- 4.1 ‡πÅ‡∏õ‡∏•‡∏á Label (String) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Binary) ---
print("\n--- 4.1 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á Labels ‡πÄ‡∏õ‡πá‡∏ô Binary (0, 1) ---")
y_train_binary = y_train.map({'positive': 1, 'negative': 0}).astype(int)
y_test_binary = y_test.map({'positive': 1, 'negative': 0}).astype(int)
 
# --- 4.2 ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hyperparameters ---
VOCAB_SIZE = 10000  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
MAX_LEN = 150       # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏µ‡∏ß‡∏¥‡∏ß
EMBEDDING_DIM = 128 # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ñ‡∏≥
 
# --- 4.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Tokenizer ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Sequences ---
print("--- 4.3 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Sequences ---")
tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)
 
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)
 
# --- 4.4 ‡∏ó‡∏≥ Padding ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å Sequence ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ---
print("--- 4.4 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥ Padding ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---")
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')
 
print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train (Padded): {X_train_pad.shape}")
print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Test (Padded): {X_test_pad.shape}")
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• 1 (Bi-LSTM)
# ==============================================================================
 
print("\n\n--- 5. ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà 1: Bidirectional LSTM ---")
 
model_bilstm = Sequential([
    Embedding(input_dim=VOCAB_SIZE,
              output_dim=EMBEDDING_DIM,
              input_length=MAX_LEN),
    SpatialDropout1D(0.2),
    Bidirectional(LSTM(units=64, return_sequences=False)),
    Dropout(0.3),
    Dense(units=32, activation='relu'),
    Dense(units=1, activation='sigmoid')
])
 
# --- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà: build ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å summary ---
model_bilstm.build(input_shape=(None, MAX_LEN))
 
print("--- Architecture ‡∏Ç‡∏≠‡∏á Bi-LSTM Model ---")
model_bilstm.summary()
 
model_bilstm.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
 
# --- ‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (Training) ---
print("\n--- 5.1 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Bi-LSTM (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà)... ---")
EPOCHS = 5
BATCH_SIZE = 64
 
history_bilstm = model_bilstm.fit(
    X_train_pad,
    y_train_binary,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_pad, y_test_binary),
    verbose=1
)
 
# --- 5.2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• Bi-LSTM ---
print("\n--- 5.2 ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• Bi-LSTM ‡∏î‡πâ‡∏ß‡∏¢ Test Dataset ---")
loss, accuracy = model_bilstm.evaluate(X_test_pad, y_test_binary, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
 
y_pred_probs_bilstm = model_bilstm.predict(X_test_pad)
y_pred_classes_bilstm = (y_pred_probs_bilstm > 0.5).astype(int).flatten()
 
print("\n--- Classification Report (Bi-LSTM) ---")
print(classification_report(y_test_binary, y_pred_classes_bilstm, target_names=['Negative', 'Positive']))
 
 
# ==============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• 2 (CNN)
# ==============================================================================
 
print("\n\n--- 6. ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà 2: CNN (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö) ---")
 
model_cnn = Sequential([
    Embedding(input_dim=VOCAB_SIZE,
              output_dim=EMBEDDING_DIM,
              input_length=MAX_LEN),
    Dropout(0.2),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(units=64, activation='relu'),
    Dropout(0.3),
    Dense(units=1, activation='sigmoid')
])
 
# --- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà: build ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å summary ---
model_cnn.build(input_shape=(None, MAX_LEN))
 
print("--- Architecture ‡∏Ç‡∏≠‡∏á CNN Model ---")
model_cnn.summary()
 
model_cnn.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
 
# --- ‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (Training) ---
print("\n--- 6.1 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà)... ---")
history_cnn = model_cnn.fit(
    X_train_pad,
    y_train_binary,
    epochs=EPOCHS,  # ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epoch ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    batch_size=BATCH_SIZE,
    validation_data=(X_test_pad, y_test_binary),
    verbose=1
)
 
# --- 6.2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ---
print("\n--- 6.2 ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏î‡πâ‡∏ß‡∏¢ Test Dataset ---")
loss_cnn, acc_cnn = model_cnn.evaluate(X_test_pad, y_test_binary, verbose=0)
print(f"Test Loss: {loss_cnn:.4f}")
print(f"Test Accuracy: {acc_cnn:.4f}")
 
y_pred_probs_cnn = model_cnn.predict(X_test_pad)
y_pred_classes_cnn = (y_pred_probs_cnn > 0.5).astype(int).flatten()
 
print("\n--- Classification Report (CNN) ---")
print(classification_report(y_test_binary, y_pred_classes_cnn, target_names=['Negative', 'Positive']))
 
print("\nüéâüéâüéâ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î! üéâüéâüéâ")
 